{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Processing jobs\n",
    "\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform.\n",
    "\n",
    "A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "<img src=\"Processing-1.jpg\">\n",
    "\n",
    "This notebook shows how you can use your own custom container to run processing jobs with your own Python libraries and dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the scikit-learn preprocessing script as a processing job, create a `ScriptProcessor`, which lets you run scripts inside of processing jobs using your custom container image.<br>\n",
    "Lets start with importing some libraries we will need and setup our input and output S3 URI's<br>\n",
    "**Please replace \\<Input S3 URI\\> with the S3 URI of the location of the source images to be processed. Note that the source images are expected to be located in a subfolder named \"abc\" from this URI.<br>\n",
    "Also replace \\<Output S3 URI\\> with the output location of the processed images. Note that the script will place the output images to subfolder \"abc\". Both subfolders are specified in the arguments when running the processing job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "#input_data = \"s3://tmp-demo-area/torchai/input/\"\n",
    "#output_data = \"s3://tmp-demo-area/torchai/output/\"\n",
    "\n",
    "input_data = \"<Input S3 URI>\"\n",
    "output_data = \"<Output S3 URI>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure input and output S3 URI's end with slash\n",
    "\n",
    "if not input_data.endswith(\"/\"):\n",
    "    input_data += \"/\"\n",
    "    \n",
    "if not output_data.endswith(\"/\"):\n",
    "    output_data += \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cell writes a file `preprocessing.py`, which contains the pre-processing script. You can update the script, and rerun this cell to overwrite `preprocessing.py`. You run this as a processing job in the next cell. This script has been modified to incorporate your preprocessing code. Parameters have been mapped to arguments that can be provided at run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\n",
    "                                 'Reads source images from an s3 bucket, '\n",
    "                                 'applies preprocessing transformations to them,'\n",
    "                                 'and saves them back to s3.')\n",
    "\n",
    "#parser.add_argument('bucket', type=str)\n",
    "parser.add_argument('--src_prefix', type=str, help='Prefix/folder for source files.')\n",
    "parser.add_argument('--dest_prefix', type=str, help='Prefix/folder for preprocessed files.')\n",
    "parser.add_argument('-is', '--img_size', type=int, help='Size to set images to. Defaults to 1800')\n",
    "parser.add_argument('-bt', '--bin_thresh', type=int, help='Binary threshold for image smoothening. Defaults to 180')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def process_img(pil_img, img_size, bin_thresh):\n",
    "    if img_size is None:\n",
    "        img_size = 1800\n",
    "    if bin_thresh is None:\n",
    "        bin_thresh = 180\n",
    "    open_cv_image = np.array(pil_img) \n",
    "    # Convert RGB to BGR \n",
    "    open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
    "    img_resized = set_image_size(open_cv_image, img_size)\n",
    "    im_new = remove_noise_and_smooth(img_resized, bin_thresh)\n",
    "    return im_new\n",
    "\n",
    "\n",
    "def set_image_size(img, img_size):\n",
    "    img = Image.fromarray(img)\n",
    "    length_x, width_y = img.size\n",
    "    factor = max(1, int(img_size / length_x))\n",
    "    size = factor * length_x, factor * width_y\n",
    "    # size = (1800, 1800)\n",
    "    img_resized = img.resize(size, Image.ANTIALIAS)\n",
    "    return img_resized\n",
    "\n",
    "\n",
    "def image_smoothening(img, bin_thresh):\n",
    "    ret1, th1 = cv2.threshold(img, bin_thresh, 255, cv2.THRESH_BINARY)\n",
    "    ret2, th2 = cv2.threshold(th1, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    blur = cv2.GaussianBlur(th2, (1, 1), 0)\n",
    "    ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return th3\n",
    "\n",
    "\n",
    "def remove_noise_and_smooth(img, bin_thresh):\n",
    "    img = np.array(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    filtered = cv2.adaptiveThreshold(img.astype(np.uint8), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 41, 3)\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    opening = cv2.morphologyEx(filtered, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    img = image_smoothening(img, bin_thresh)\n",
    "    or_image = cv2.bitwise_or(img, closing)\n",
    "    return or_image\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", args.src_prefix)\n",
    "    output_data_path = os.path.join(\"/opt/ml/processing/output\", args.dest_prefix)\n",
    "    \n",
    "    print(f\"Input Data Path {input_data_path}\")\n",
    "    print(f\"Output Data Path {output_data_path}\")\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_data_path)\n",
    "    except OSError as error:\n",
    "        print(error) \n",
    "    \n",
    "    for img_obj in os.listdir(input_data_path):\n",
    "        if img_obj.lower().endswith('.jpg'):\n",
    "            # process image\n",
    "            img_fname = f\"{input_data_path}/{img_obj}\"\n",
    "            img = Image.open(img_fname)\n",
    "            processed_img = process_img(img, args.img_size, args.bin_thresh)\n",
    "\n",
    "            # save preprocessed image back to s3\n",
    "            img_filename = img_obj.split('/')[-1]\n",
    "            page_img_obj = f'{output_data_path}/{img_filename}'\n",
    "            Image.fromarray(processed_img).save(page_img_obj, format='JPEG')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy our sample image to your S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp sample.JPG {input_data}abc/sample.JPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running processing jobs with your own dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's copy the previously created `preprocessing.py` script to `code` folder for next `run` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code\n",
    "!cp preprocessing.py ./code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you walk through how to create a processing container, and how to use a `ScriptProcessor` to run your own code within a container.  You can provide your own dependencies inside this container to run your processing script with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Dockerfile to create the processing container. Install `opencv`,`tqdm` and `Pillow` into it. You can install your own dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN apt-get update && apt-get install -y python3-opencv\n",
    "RUN pip3 install Pillow opencv-python tqdm\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code builds the container using the `docker` command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "ecr_repository = \"sagemaker-processing-container\"\n",
    "tag = \":latest\"\n",
    "\n",
    "uri_suffix = \"amazonaws.com\"\n",
    "if region in [\"cn-north-1\", \"cn-northwest-1\"]:\n",
    "    uri_suffix = \"amazonaws.com.cn\"\n",
    "processing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "    account_id, region, uri_suffix, ecr_repository + tag\n",
    ")\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class lets you run a command inside this container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=processing_repository_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same `preprocessing.py` script you created above, inside of the Docker container you built in this notebook. You can add the dependencies to the Docker image, and run your own pre-processing, feature-engineering, and model evaluation scripts inside of this container. We provide the run method `SKLearnProcessor.run()` the S3 input location through `ProcessingInput`, the S3 output location using `ProcessingOutput` and the associated arguments to run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    inputs=[ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source=\"/opt/ml/processing/output\", destination=output_data)\n",
    "    ],\n",
    "    arguments=[\"--img_size\", \"1700\", \"--src_prefix\", \"abc\", \"--dest_prefix\", \"abc\"],\n",
    ")\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "print(script_processor_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script will process images located in your S3 input location from a subfolder named \"abc\" (src_prefix) and will put to processed images to the output location in a sub folder named \"abc\" (dest_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You saw how to create a processing container, and how to use a `ScriptProcessor` to run your own code within a container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
